/home/yl9315/.bashrc: line 13: export: `PATHÃ¸': not a valid identifier
srun: fatal: No command given to execute.
Some weights of the model checkpoint at brad1141/gpt2-finetuned-comp2 were not used when initializing GPT2ForTokenClassification: ['transformer.h.5.attn.bias', 'transformer.h.8.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.0.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.2.attn.bias', 'transformer.h.9.attn.bias', 'transformer.h.11.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.7.attn.bias']
- This IS expected if you are initializing GPT2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at brad1141/gpt2-finetuned-comp2 and are newly initialized because the shapes did not match:
- transformer.wpe.weight: found shape torch.Size([1024, 768]) in the checkpoint and torch.Size([2048, 768]) in the model instantiated
- classifier.weight: found shape torch.Size([7, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated
- classifier.bias: found shape torch.Size([7]) in the checkpoint and torch.Size([4]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
GPT2TokenizerFast(name_or_path='brad1141/gpt2-finetuned-comp2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)
GPT2Config {
  "_name_or_path": "brad1141/gpt2-finetuned-comp2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2ForTokenClassification"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 2048,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.30.2",
  "use_cache": true,
  "vocab_size": 50257
}

GPT2ForTokenClassification(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(2048, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
)
Epoch 1/20, Loss: 1.1367
Epoch 2/20, Loss: 1.0696
Epoch 3/20, Loss: 1.0600
Epoch 4/20, Loss: 1.0541
Epoch 5/20, Loss: 1.0524
Epoch 6/20, Loss: 1.0482
Epoch 7/20, Loss: 1.0485
Epoch 8/20, Loss: 1.0478
Epoch 9/20, Loss: 1.0470
Epoch 10/20, Loss: 1.0458
Epoch 11/20, Loss: 1.0438
Epoch 12/20, Loss: 1.0441
Epoch 13/20, Loss: 1.0416
Epoch 14/20, Loss: 1.0411
Epoch 15/20, Loss: 1.0415
Epoch 16/20, Loss: 1.0413
Epoch 17/20, Loss: 1.0396
Epoch 18/20, Loss: 1.0420
Epoch 19/20, Loss: 1.0409
Epoch 20/20, Loss: 1.0378
